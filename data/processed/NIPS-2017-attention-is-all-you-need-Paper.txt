

-------- PAGE 1 --------

Attention Is All You Need
AshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗
GoogleBrain GoogleBrain GoogleResearch GoogleResearch
avaswani@google.com noam@google.com nikip@google.com usz@google.com
LlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗
GoogleResearch UniversityofToronto GoogleBrain
llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com
IlliaPolosukhin∗ ‡
illia.polosukhin@gmail.com
Abstract
Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor
convolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions
entirely. Experiments on two machine translation tasks show these models to
besuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-
to-German translation task, improving over the existing best results, including
ensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,
ourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.0after
trainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe
bestmodelsfromtheliterature.
1 Introduction
Recurrentneuralnetworks,longshort-termmemory[12]andgatedrecurrent[7]neuralnetworks
inparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand
transductionproblemssuchaslanguagemodelingandmachinetranslation[29,2,5]. Numerous
effortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder
architectures[31,21,13].
∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted
theefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand
hasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head
attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery
detail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand
tensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and
efficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand
implementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating
ourresearch.
†WorkperformedwhileatGoogleBrain.
‡WorkperformedwhileatGoogleResearch.
31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.

-------- PAGE 2 --------

Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput
sequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden
statesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently
t t−1
sequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger
sequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved
significantimprovementsincomputationalefficiencythroughfactorizationtricks[18]andconditional
computation[26],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental
constraintofsequentialcomputation,however,remains.
Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-
tionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein
theinputoroutputsequences[2,16]. Inallbutafewcases[22],however,suchattentionmechanisms
areusedinconjunctionwitharecurrentnetwork.
InthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead
relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.
TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin
translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.
2 Background
ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU
[20],ByteNet[15]andConvS2S[8],allofwhichuseconvolutionalneuralnetworksasbasicbuilding
block,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,
thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows
inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes
it more difficult to learn dependencies between distant positions [11]. In the Transformer this is
reducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
describedinsection3.2.
Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions
ofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen
usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,
textualentailmentandlearningtask-independentsentencerepresentations[4,22,23,19].
End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-
alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand
languagemodelingtasks[28].
To the best of our knowledge, however, the Transformer is the first transduction model relying
entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-
alignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate
self-attentionanddiscussitsadvantagesovermodelssuchas[14,15]and[8].
3 ModelArchitecture
Mostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,29].
Here, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence
1 n
of continuous representations z = (z ,...,z ). Given z, the decoder then generates an output
1 n
sequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive
1 m
[9],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.
TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully
connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,
respectively.
3.1 EncoderandDecoderStacks
Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two
sub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-
2

-------- PAGE 3 --------

Figure1: TheTransformer-modelarchitecture.
wisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[10]aroundeachof
the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is
LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer
itself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding
layers,produceoutputsofdimensiond =512.
model
Decoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo
sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head
attentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections
aroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe
predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.
3.2 Attention
Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,
wherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum
ofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe
querywiththecorrespondingkey.
3.2.1 ScaledDot-ProductAttention
Wecallourparticularattention"ScaledDot-ProductAttention"(Figure2). Theinputconsistsof
queriesandkeysofdimensiond ,andvaluesofdimensiond . Wecomputethedotproductsofthe
k v
3

-------- PAGE 4 --------

ScaledDot-ProductAttention Multi-HeadAttention
Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
attentionlayersrunninginparallel.
√
querywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe
k
values.
Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether
intoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute
thematrixofoutputsas:
QKT
Attention(Q,K,V)=softmax( √ )V (1)
d
k
Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-
plicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor
of √1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith
dk
asinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis
muchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized
matrixmultiplicationcode.
Whileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms
k
dotproductattentionwithoutscalingforlargervaluesofd [3]. Wesuspectthatforlargevaluesof
k
d ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas
k
extremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby √1 .
dk
3.2.2 Multi-HeadAttention
Insteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries,
model
wefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned
linearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof
k k v
queries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional
v
output values. These are concatenated and once again projected, resulting in the final values, as
depictedinFigure2.
Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation
subspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis.
4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom
variableswithmean0andvariance1.Thentheirdotproduct,q·k=
(cid:80)dk
q k ,hasmean0andvarianced .
i=1 i i k
4

-------- PAGE 5 --------

MultiHead(Q,K,V)=Concat(head ,...,head )WO
1 h
wherehead =Attention(QWQ,KWK,VWV)
i i i i
WheretheprojectionsareparametermatricesWQ ∈Rdmodel×dk,WK ∈Rdmodel×dk,WV ∈Rdmodel×dv
i i i
andWO ∈Rhdv×dmodel.
In this work we employ h = 8 parallel attention layers, or heads. For each of these we use
d =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost
k v model
issimilartothatofsingle-headattentionwithfulldimensionality.
3.2.3 ApplicationsofAttentioninourModel
TheTransformerusesmulti-headattentioninthreedifferentways:
• In"encoder-decoderattention"layers,thequeriescomefromthepreviousdecoderlayer,
andthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery
positioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as
[31,2,8].
• Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values
andqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe
encoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe
encoder.
• Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto
allpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward
informationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis
insideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput
ofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2.
3.3 Position-wiseFeed-ForwardNetworks
Inadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully
connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This
consistsoftwolineartransformationswithaReLUactivationinbetween.
FFN(x)=max(0,xW +b )W +b (2)
1 1 2 2
Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is d = 512, and the inner-layer has dimensionality
model
d =2048.
ff
3.4 EmbeddingsandSoftmax
Similarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput
tokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor-
model
mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In
ourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax
lineartransformation,similarto[24]. Intheembeddinglayers,wemultiplythoseweightsby d .
model
3.5 PositionalEncoding
Sinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe
orderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe
tokensinthesequence. Tothisend,weadd"positionalencodings"totheinputembeddingsatthe
5

-------- PAGE 6 --------

Table1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations
fordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel
sizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention.
LayerType ComplexityperLayer Sequential MaximumPathLength
Operations
Self-Attention O(n2·d) O(1) O(1)
Recurrent O(n·d2) O(n) O(n)
Convolutional O(k·n·d2) O(1) O(log (n))
k
Self-Attention(restricted) O(r·n·d) O(1) O(n/r)
bottomsoftheencoderanddecoderstacks. Thepositionalencodingshavethesamedimensiond
model
astheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings,
learnedandfixed[8].
Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies:
PE =sin(pos/100002i/dmodel)
(pos,2i)
PE =cos(pos/100002i/dmodel)
(pos,2i+1)
whereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding
correspondstoasinusoid. Thewavelengthsformageometricprogressionfrom2πto10000·2π. We
chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby
relativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof
pos+k
PE .
pos
Wealsoexperimentedwithusinglearnedpositionalembeddings[8]instead,andfoundthatthetwo
versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion
becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered
duringtraining.
4 WhySelf-Attention
In this section we compare various aspects of self-attention layers to the recurrent and convolu-
tionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations
(x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z ∈ Rd, such as a hidden
1 n 1 n i i
layerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe
considerthreedesiderata.
Oneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan
beparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired.
Thethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range
dependenciesisakeychallengeinmanysequencetransductiontasks. Onekeyfactoraffectingthe
abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto
traverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput
andoutputsequences,theeasieritistolearnlong-rangedependencies[11]. Hencewealsocompare
themaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe
differentlayertypes.
AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially
executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of
computationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence
length n is smaller than the representation dimensionality d, which is most often the case with
sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece
[31]andbyte-pair[25]representations. Toimprovecomputationalperformancefortasksinvolving
verylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin
6

-------- PAGE 7 --------

theinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum
pathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework.
Asingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput
positions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels,
orO(log (n))inthecaseofdilatedconvolutions[15], increasingthelengthofthelongestpaths
k
betweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan
recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity
considerably, toO(k·n·d+n·d2). Evenwithk = n, however, thecomplexityofaseparable
convolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer,
theapproachwetakeinourmodel.
Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions
fromourmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention
headsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic
andsemanticstructureofthesentences.
5 Training
Thissectiondescribesthetrainingregimeforourmodels.
5.1 TrainingDataandBatching
We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
sentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-
targetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT
2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece
vocabulary[31].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining
batchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000
targettokens.
5.2 HardwareandSchedule
Wetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing
thehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We
trainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe
bottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps
(3.5days).
5.3 Optimizer
WeusedtheAdamoptimizer[17]withβ =0.9,β =0.98and(cid:15)=10−9. Wevariedthelearning
1 2
rateoverthecourseoftraining,accordingtotheformula:
lrate=d−0.5 ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)
model
Thiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps,
anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused
warmup_steps=4000.
5.4 Regularization
Weemploythreetypesofregularizationduringtraining:
ResidualDropout Weapplydropout[27]totheoutputofeachsub-layer,beforeitisaddedtothe
sub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe
positionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof
P =0.1.
drop
7

-------- PAGE 8 --------

Table2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe
English-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.
BLEU TrainingCost(FLOPs)
Model
EN-DE EN-FR EN-DE EN-FR
ByteNet[15] 23.75
Deep-Att+PosUnk[32] 39.2 1.0·1020
GNMT+RL[31] 24.6 39.92 2.3·1019 1.4·1020
ConvS2S[8] 25.16 40.46 9.6·1018 1.5·1020
MoE[26] 26.03 40.56 2.0·1019 1.2·1020
Deep-Att+PosUnkEnsemble[32] 40.4 8.0·1020
GNMT+RLEnsemble[31] 26.30 41.16 1.8·1020 1.1·1021
ConvS2SEnsemble[8] 26.36 41.29 7.7·1019 1.2·1021
Transformer(basemodel) 27.3 38.1 3.3·1018
Transformer(big) 28.4 41.0 2.3·1019
LabelSmoothing Duringtraining,weemployedlabelsmoothingofvalue(cid:15) = 0.1[30]. This
ls
hurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.
6 Results
6.1 MachineTranslation
OntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big)
inTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0
BLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis
listedinthebottomlineofTable3. Trainingtook3.5dayson8P100GPUs. Evenourbasemodel
surpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof
thecompetitivemodels.
OntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0,
outperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe
previousstate-of-the-artmodel. TheTransformer(big)modeltrainedforEnglish-to-Frenchused
dropoutrateP =0.1,insteadof0.3.
drop
Forthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which
werewrittenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We
usedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[31]. Thesehyperparameters
werechosenafterexperimentationonthedevelopmentset. Wesetthemaximumoutputlengthduring
inferencetoinputlength+50,butterminateearlywhenpossible[31].
Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel
architecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina
modelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained
single-precisionfloating-pointcapacityofeachGPU5.
6.2 ModelVariations
ToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel
indifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe
developmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno
checkpointaveraging. WepresenttheseresultsinTable3.
InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,
keeping the amount of computation constant, as described in Section 3.2.2. While single-head
attentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.
5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.
8

-------- PAGE 9 --------

Table3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase
model. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. Listed
perplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto
per-wordperplexities.
train PPL BLEU params
N d d h d d P (cid:15)
model ff k v drop ls steps (dev) (dev) ×106
base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65
1 512 512 5.29 24.9
4 128 128 5.00 25.5
(A)
16 32 32 4.91 25.8
32 16 16 5.01 25.4
16 5.16 25.1 58
(B)
32 5.01 25.4 60
2 6.11 23.7 36
4 5.19 25.3 50
8 4.88 25.5 80
(C) 256 32 32 5.75 24.5 28
1024 128 128 4.66 26.0 168
1024 5.12 25.4 53
4096 4.75 26.2 90
0.0 5.77 24.6
0.2 4.95 25.5
(D)
0.0 4.67 25.3
0.2 5.47 25.7
(E) positionalembeddinginsteadofsinusoids 4.92 25.7
big 6 1024 4096 16 0.3 300K 4.33 26.4 213
InTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This
k
suggests that determining compatibility is not easy and that a more sophisticated compatibility
functionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected,
biggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour
sinusoidalpositionalencodingwithlearnedpositionalembeddings[8],andobservenearlyidentical
resultstothebasemodel.
7 Conclusion
Inthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon
attention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith
multi-headedself-attention.
For translation tasks, the Transformer can be trained significantly faster than architectures based
on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014
English-to-Frenchtranslationtasks,weachieveanewstateoftheart. Intheformertaskourbest
modeloutperformsevenallpreviouslyreportedensembles.
Weareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks. We
plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand
toinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs
suchasimages,audioandvideo. Makinggenerationlesssequentialisanotherresearchgoalsofours.
The code we used to train and evaluate our models is available at https://github.com/
tensorflow/tensor2tensor.
Acknowledgements Wearegratefulto NalKalchbrennerand StephanGouwsfor theirfruitful
comments,correctionsandinspiration.
9

-------- PAGE 10 --------

References
[1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint
arXiv:1607.06450,2016.
[2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly
learningtoalignandtranslate. CoRR,abs/1409.0473,2014.
[3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural
machinetranslationarchitectures. CoRR,abs/1703.03906,2017.
[4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine
reading. arXivpreprintarXiv:1601.06733,2016.
[5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,
andYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderforstatistical
machinetranslation. CoRR,abs/1406.1078,2014.
[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv
preprintarXiv:1610.02357,2016.
[7] JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation
ofgatedrecurrentneuralnetworksonsequencemodeling. CoRR,abs/1412.3555,2014.
[8] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu-
tionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122v2,2017.
[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850,2013.
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-
age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition,pages770–778,2016.
[11] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber. Gradientflowin
recurrentnets: thedifficultyoflearninglong-termdependencies,2001.
[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735–1780,1997.
[13] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring
thelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016.
[14] ŁukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference
onLearningRepresentations(ICLR),2016.
[15] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-
rayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2,
2017.
[16] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks.
InInternationalConferenceonLearningRepresentations,2017.
[17] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015.
[18] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint
arXiv:1703.10722,2017.
[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen
Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint
arXiv:1703.03130,2017.
[20] SamyBengioŁukaszKaiser. Canactivememoryreplaceattention? InAdvancesinNeural
InformationProcessingSystems,(NIPS),2016.
10

-------- PAGE 11 --------

[21] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention-
basedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015.
[22] AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. Adecomposableattention
model. InEmpiricalMethodsinNaturalLanguageProcessing,2016.
[23] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive
summarization. arXivpreprintarXiv:1705.04304,2017.
[24] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv
preprintarXiv:1608.05859,2016.
[25] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords
withsubwordunits. arXivpreprintarXiv:1508.07909,2015.
[26] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,
andJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts
layer. arXivpreprintarXiv:1701.06538,2017.
[27] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-
nov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine
LearningResearch,15(1):1929–1958,2014.
[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory
networks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors,
AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,
Inc.,2015.
[29] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural
networks. InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014.
[30] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.
Rethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015.
[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google’sneuralmachine
translationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint
arXiv:1609.08144,2016.
[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with
fast-forwardconnectionsforneuralmachinetranslation. CoRR,abs/1606.04199,2016.
11